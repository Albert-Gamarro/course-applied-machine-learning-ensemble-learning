{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ensemble Learning\n",
    "## Definition of Ensemble Learning and the Problem of Overfitting\n",
    "Ensemble learning is a powerful machine learning paradigm where multiple models are trained to solve the same problem and combined to obtain better results. It is particularly important for improving the accuracy and robustness of models and overcoming issues like overfitting, where a model learns the training data too well but fails to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Real-world Relevance\n",
    "Ensemble learning is widely used in real-world applications for its robustness and accuracy. For example, it is used in finance for risk prediction, in healthcare for disease detection, and in e-commerce for recommendation systems. The main advantage is its ability to harness the power of multiple models to provide better predictions than individual models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Types of Ensembles\n",
    "There are several types of ensemble methods, including:\n",
    "1. **Bagging**: Builds multiple models independently and combines them by averaging predictions (e.g., Random Forests).\n",
    "2. **Boosting**: Builds models sequentially, with each model trying to correct the mistakes of its predecessor (e.g., AdaBoost, Gradient Boosting).\n",
    "3. **Stacking**: Combines multiple models using a meta-model to improve predictions.\n",
    "Each type of ensemble has its own method of model integration, with varying degrees of complexity and computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bagging and Random Forests\n",
    "\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is an ensemble technique that aims to improve the accuracy and stability of machine learning algorithms. It involves creating multiple subsets of a dataset with replacement (bootstrap) and training a model on each subset. The final prediction is obtained by averaging the predictions (for regression) or voting (for classification). Bagging helps reduce variance, thereby mitigating the risk of overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data (don't need to run this)\n",
    "\n",
    "I used `drawdata` to generate the data for the *Explore Bagging* section. It is a useful library to create simple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drawdata\n",
    "\n",
    "widget = drawdata.ScatterWidget()\n",
    "widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = widget.data_as_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.to_csv('data/simple.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 6 different random forests with a single tree using different seed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layouts = '''\n",
    "123\n",
    "456\n",
    "'''\n",
    "fig, axs = plt.subplot_mosaic(layouts, figsize=(8, 4))\n",
    "\n",
    "n_trees = [1]*6\n",
    "data = pd.read_csv('data/simple.csv')\n",
    "for i, n_tree in enumerate(n_trees, start=1):\n",
    "    ax = axs[str(i)]\n",
    "    data.plot.scatter(x='x', y='y', ax=ax, title=f'n_trees={n_tree} seed={i}', s=1)\n",
    "    # change random_state to get different trees\n",
    "    clf = RandomForestRegressor(n_estimators=n_tree, max_depth=3, max_samples=.01, random_state=i)\n",
    "    clf.fit(data[['x']], data['y'])\n",
    "    x = pd.DataFrame({'x': np.linspace(data.x.min(), data.x.max(), 100)})\n",
    "    y = clf.predict(x)\n",
    "    ax.plot(x, y, c='red')\n",
    "\n",
    "fig.tight_layout()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate random forest vs single tree\n",
    "# note that making max_samples very small is a way to make the trees more different\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layouts = '''\n",
    "123\n",
    "456\n",
    "'''\n",
    "fig, axs = plt.subplot_mosaic(layouts, figsize=(8, 4))\n",
    "\n",
    "n_trees = [1, 2, 5, 10, 20 , 50]\n",
    "data = pd.read_csv('data/simple.csv')\n",
    "for i, n_tree in enumerate(n_trees, start=1):\n",
    "    ax = axs[str(i)]\n",
    "    data.plot.scatter(x='x', y='y', ax=ax, title=f'n_trees={n_tree}', s=1)\n",
    "    # play with max_depth and max_samples to see how they affect the trees\n",
    "    rf = RandomForestRegressor(n_estimators=n_tree, max_depth=3, max_samples=.01, random_state=42)\n",
    "    rf.fit(data[['x']], data['y'])\n",
    "    x = pd.DataFrame({'x': np.linspace(data.x.min(), data.x.max(), 100)})\n",
    "\n",
    "    # loop over trees and plot them\n",
    "    for i, tree in enumerate(rf.estimators_):\n",
    "        y = tree.predict(x)\n",
    "        ax.plot(x, y, c='grey', alpha=.9)\n",
    "    y = rf.predict(x)\n",
    "    ax.plot(x, y, c='red')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def plot_random_forest(data_path, max_depth, max_samples):\n",
    "  \"\"\"\n",
    "  Plots random forest predictions for different numbers of trees.\n",
    "\n",
    "  Args:\n",
    "      data_path (str): Path to the CSV file containing data.\n",
    "      max_depth (int): Maximum depth of the trees.\n",
    "      max_samples (float): Proportion of samples to draw for each tree.\n",
    "  \"\"\"\n",
    "  layouts = '''\n",
    "  123456\n",
    "  '''\n",
    "  plt.close('all')\n",
    "  fig, axs = plt.subplot_mosaic(layouts, figsize=(18, 4))\n",
    "  n_trees = [1, 2, 5, 10, 20, 50]\n",
    "  data = pd.read_csv(data_path)\n",
    "  for i, n_tree in enumerate(n_trees, start=1):\n",
    "      ax = axs[str(i)]\n",
    "      data.plot.scatter(x='x', y='y', ax=ax, title=f'n_trees={n_tree}', s=1)\n",
    "\n",
    "      rf = RandomForestRegressor(n_estimators=n_tree, \n",
    "                                 max_depth=max_depth, \n",
    "                                 max_samples=max_samples, \n",
    "                                 random_state=42)\n",
    "      rf.fit(data[['x']], data['y'])\n",
    "      x = pd.DataFrame({'x': np.linspace(data.x.min(), data.x.max(), 100)})\n",
    "\n",
    "      # loop over trees and plot them\n",
    "      for i, tree in enumerate(rf.estimators_):\n",
    "          y = tree.predict(x)\n",
    "          ax.plot(x, y, c='grey', alpha=.9)\n",
    "      y = rf.predict(x)\n",
    "      ax.plot(x, y, c='red')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# Define sliders\n",
    "max_depth_slider = IntSlider(min=1, max=10, value=3, description=\"Max Depth\")\n",
    "max_samples_slider = FloatSlider(min=0.01, max=1.0, step=0.01, value=0.1, description=\"Max Samples\")\n",
    "\n",
    "# Update the plot whenever the slider values change\n",
    "interact(plot_random_forest, \n",
    "         data_path=\"data/simple.csv\", \n",
    "         max_depth=max_depth_slider, \n",
    "         max_samples=max_samples_slider)\n",
    "\n",
    "# as we raise the depth, the trees become more complex\n",
    "# as we raise the samples, the prediction becomes smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Example\n",
    "\n",
    "A Random Forest is a popular ensemble method that utilizes bagging. It builds a multitude of decision trees at training time and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. It is known for its robustness and ease of use without extensive hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(transform_output='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "adult_data = fetch_openml(data_id=1590)  # Adult census income dataset\n",
    "X = adult_data.data\n",
    "y = adult_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local parquet file\n",
    "(X\n",
    " .assign(y=y)\n",
    " .to_parquet('data/adult.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = pd.read_parquet('data/adult.parquet')\n",
    "X = (adult.drop(columns='y'))\n",
    "y = adult.y\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out my Applied Machine Learning: Feature Engineering course for more details\n",
    "# make pipeline to handle categorical data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['category']).columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a technique called one-hot encoding (or dummy encoding) to encode the categorical variables\n",
    "# Note that age is numerical so we don't want to apply one-hot encoding to it\n",
    "categorical_transformer.fit_transform(X.iloc[:, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only encode the categorical features\n",
    "categorical_transformer.fit_transform(X[categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try just a decision tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# pipeline with decision tree\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Model Accuracy: {accuracy:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# pipeline with random forest\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Model Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning for Random Forest\n",
    "\n",
    "While Random Forest is quite robust, tuning its hyperparameters can significantly boost performance. Key parameters include the number of trees (n_estimators), the maximum depth of each tree (max_depth), and the number of features to consider for each split (max_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune number of trees\n",
    "import pandas as pd\n",
    "\n",
    "scores = []\n",
    "training_scores = []\n",
    "n_estimators = [10, 50, 100, 150, 200, 250, 300]\n",
    "for n in n_estimators:\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=n, random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    scores.append(accuracy) \n",
    "    training_predictions = pipeline.predict(X_train)\n",
    "    training_accuracy = accuracy_score(y_train, training_predictions)\n",
    "    training_scores.append(training_accuracy)\n",
    "\n",
    "(pd.DataFrame({'n_estimators': n_estimators, 'accuracy': scores, 'training_accuracy': training_scores})\n",
    " .set_index('n_estimators')\n",
    "    .plot(title='Random Forest Classifier Accuracy vs Number of Trees', ylabel='Accuracy'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge: Tune Random Forest Parameters\n",
    "\n",
    "Task: Improve the model accuracy by tuning the 'max_depth' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Solution: Tune Random Forest Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boosting and Gradient Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Concept of Boosting\n",
    "\n",
    "The key idea in boosting is to train models sequentially. With each model, the algorithm attempts to fix the errors made by the previous models. The base models are often simple, like decision stumps, and combined to form a stronger learner. Boosting is particularly effective in reducing bias and variance in the model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost: A Simple Boosting Example\n",
    "\n",
    "AdaBoost (Adaptive Boosting) is a popular boosting algorithm. It is a meta-learner and can use different base learners.\n",
    "We will use a Decision Tree Classifier as the base learner for AdaBoost in this example.\n",
    "\n",
    "- First, it trains a model\n",
    "- For subsequent models, it focuses more on the misclassified samples from the previous model (gives them a larger weight)\n",
    "\n",
    "The final prediction is a weighted sum of the predictions from all the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate random forest vs single tree\n",
    "# note that making max_samples very small is a way to \n",
    "# make the trees more different\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layouts = '''\n",
    "123\n",
    "456\n",
    "'''\n",
    "fig, axs = plt.subplot_mosaic(layouts, figsize=(10, 4))\n",
    "\n",
    "n_trees = [1, 2, 5, 10, 20 , 50]\n",
    "data = pd.read_csv('data/simple.csv')\n",
    "for i, n_tree in enumerate(n_trees, start=1):\n",
    "    ax = axs[str(i)]\n",
    "    data.plot.scatter(x='x', y='y', ax=ax, title=f'n_trees={n_tree}', s=1)\n",
    "    # play with max_depth and max_samples to see how they affect the trees\n",
    "    clf = AdaBoostRegressor(n_estimators=n_tree, estimator=DecisionTreeRegressor(max_depth=3), random_state=42)\n",
    "    #clf = AdaBoostRegressor(n_estimators=n_tree, random_state=42, estimator=LinearRegression())\n",
    "    clf.fit(data[['x']], data['y'])\n",
    "    x = pd.DataFrame({'x': np.linspace(data.x.min(), data.x.max(), 100)})\n",
    "\n",
    "    # plot just the last tree in grey\n",
    "    y = clf.estimators_[-1].predict(x)\n",
    "    ax.plot(x, y, c='grey', alpha=.9)\n",
    "    # plot the final prediction in red\n",
    "    y = clf.predict(x)\n",
    "    ax.plot(x, y, c='red')\n",
    "\n",
    "fig.tight_layout()  \n",
    "\n",
    "# note that in tree 2 the grey line is fixing the left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', AdaBoostClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, pipeline.predict(X_test))\n",
    "print(f'Model Accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient Boosting is another boosting algorithm that uses a gradient descent approach to minimize the loss of the combined model. \n",
    "Rather than focusing on the misclassified samples, Gradient Boosting fits the new model to the residual errors of the previous model.\n",
    "Each new model aims to correct the mistakes of the previous ones by moving in the direction of the gradient that reduces loss.\n",
    "\n",
    "In Gradient Boosting, decision trees are trained sequentially to fit the gradient of the loss function, perfecting errors from previous models. It's powerful for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate random forest vs single tree\n",
    "# note that making max_samples very small is a way to make the trees more different\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layouts = '''\n",
    "123\n",
    "456\n",
    "'''\n",
    "fig, axs = plt.subplot_mosaic(layouts, figsize=(10, 4))\n",
    "\n",
    "n_trees = [1, 2, 5, 10, 20 , 50]\n",
    "data = pd.read_csv('data/simple.csv')\n",
    "for i, n_tree in enumerate(n_trees, start=1):\n",
    "    ax = axs[str(i)]\n",
    "    data.plot.scatter(x='x', y='y', ax=ax, title=f'n_trees={n_tree}', s=1)\n",
    "    # play with max_depth and max_samples to see how they affect the trees\n",
    "    clf = GradientBoostingRegressor(n_estimators=n_tree, random_state=42, max_depth=3)\n",
    "    clf.fit(data[['x']], data['y'])\n",
    "    x = pd.DataFrame({'x': np.linspace(data.x.min(), data.x.max(), 100)})\n",
    "    # plot just the last tree in grey\n",
    "    # GradientBoostingRegressor stores the trees in a numpy array\n",
    "    y = clf.estimators_.flatten()[-1].predict(x)\n",
    "    ax.plot(x, y, c='grey', alpha=.9)\n",
    "    # plot the final prediction in red\n",
    "    y = clf.predict(x)\n",
    "    ax.plot(x, y, c='red')\n",
    "\n",
    "fig.tight_layout()  \n",
    "\n",
    "# the scikit learn implementation uses a dummy regressor initially\n",
    "# each tree corrects the previous (thats why the grey line is bounding 0)\n",
    "# You want to use \"weak learners\" so that you don't overfit (try changing the max_depth to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', GradientBoostingClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(f'Model Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning for Boosting Models\n",
    "Tuning hyperparameters is critical for maximizing the performance of boosting models.\n",
    "\n",
    "Hyperparameters for AdaBoost include:\n",
    "\n",
    "- `n_estimators`: Number of boosting stages to be run. (1, inf)\n",
    "- `learning_rate`: Contribution of each model to the final prediction. (number between 0 and 1)\n",
    "- `estimator`: The base estimator to fit on the training data. (Can set additional hyperparameters on the base estimator)\n",
    "\n",
    "Key hyperparameters for Gradient Boosting include:\n",
    "\n",
    "- `learning_rate`: Contribution of each tree to the final prediction. (0, 1]\n",
    "- `n_estimators`: Number of boosting stages to be run. (1, inf)\n",
    "- `max_depth`: Maximum depth of the individual trees. (1, inf)\n",
    "- `subsample`: Fraction of samples used to fit the individual base learners. (0, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune number of trees\n",
    "import pandas as pd\n",
    "\n",
    "scores = []\n",
    "training_scores = []\n",
    "n_estimators = [1, 5, 10, 20, 50, 70, 100, 150, 200]\n",
    "for n in n_estimators:\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', GradientBoostingClassifier(n_estimators=n, random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    scores.append(accuracy) \n",
    "    training_predictions = pipeline.predict(X_train)\n",
    "    training_accuracy = accuracy_score(y_train, training_predictions)\n",
    "    training_scores.append(training_accuracy)\n",
    "\n",
    "(pd.DataFrame({'n_estimators': n_estimators, 'accuracy': scores, 'training_accuracy': training_scores})\n",
    " .set_index('n_estimators')\n",
    "    .plot(title='Gradient Boosting Classifier Accuracy vs Number of Trees', ylabel='Accuracy'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge: Tune AdaBoost Model\n",
    "\n",
    "Your task is to optimize the `max_depth` parameter of an AdaBoost model to achieve higher accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Tune AdaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  XGBoost\n",
    "\n",
    "XGBoost, short for eXtreme Gradient Boosting, is an advanced implementation of gradient boosting developed by Tianqi Chen. It is known for its efficiency, speed, and predictive accuracy, making it a popular choice among data scientists.\n",
    "\n",
    "\n",
    "Why use XGBoost vs the Gradient Boosting Classifier in scikit-learn?\n",
    "- XGBoost is specifically designed to optimize computational speed and model performance.\n",
    "- Has regularization techniques to avoid overfitting.\n",
    "- Supports parallel processing and distributed computing for large datasets.\n",
    "- Provides a variety of objective functions and evaluation criteria for regression, classification, and ranking problems.\n",
    "- Handles missing data.\n",
    "- Handles categorical features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on Coding with XGBoost\n",
    "\n",
    "To start using XGBoost, ensure that the XGBoost library is installed in your Python environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate random forest vs single tree\n",
    "# note that making max_samples very small is a way to make the trees more different\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layouts = '''\n",
    "123\n",
    "456\n",
    "'''\n",
    "fig, axs = plt.subplot_mosaic(layouts, figsize=(8, 4))\n",
    "\n",
    "n_trees = [1, 2, 5, 10, 20 , 50]\n",
    "data = pd.read_csv('data/simple.csv')\n",
    "for i, n_tree in enumerate(n_trees, start=1):\n",
    "    ax = axs[str(i)]\n",
    "    data.plot.scatter(x='x', y='y', ax=ax, title=f'n_trees={n_tree}', s=1)\n",
    "    # play with max_depth and max_samples to see how they affect the trees\n",
    "    rf = XGBRegressor(n_estimators=n_tree, #estimator=DecisionTreeRegressor(max_depth=3),\n",
    "                      max_depth=2,\n",
    "                       random_state=42)\n",
    "    rf.fit(data[['x']], data['y'])\n",
    "    x = pd.DataFrame({'x': np.linspace(data.x.min(), data.x.max(), 100)})\n",
    "\n",
    "    # loop over trees and plot them\n",
    "    for z, j in enumerate(range(n_tree)):\n",
    "        y = rf.predict(x, iteration_range=(j, j+1))\n",
    "        # increase alpha based on z to make latter trees more visible\n",
    "        ax.plot(x, y, c='grey', alpha=(z+1)/n_tree)\n",
    "    y = rf.predict(x)\n",
    "    # plot the final prediction in red\n",
    "    ax.plot(x, y, c='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# you can see n tree = 2 the right side is fixed by the second tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no pipeline needed! it handles categorical data (and missing values) internally\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(enable_categorical=True)\n",
    "model.fit(X_train, y_train=='>50K')\n",
    "accuracy = accuracy_score(y_test=='>50K', model.predict(X_test))\n",
    "print(f'Model Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning for XGBoost\n",
    "XGBoost is rich in hyperparameters that, when tuned properly, can significantly enhance model performance. The primary tuning parameters include:\n",
    "- `max_depth`: Maximum depth of a tree. Increasing this value makes the model more complex and, hence, more likely to overfit. [0, inf)\n",
    "- `eta`: Step size shrinkage is used in the update to prevent overfitting. (0, 1]\n",
    "- `subsample`: Proportion of training instances used in trees; reduces overfitting. (0, 1]\n",
    "- `colsample_bytree`: The fraction of features used from each column helps with overfitting. (0, 1]\n",
    "- `gamma`: Minimum loss reduction required to make a further partition on a leaf node of the tree. L0 regularization term. [0, inf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "training_scores = []\n",
    "values = [1, 2, 3, 6, 10, 15]\n",
    "for value in values:\n",
    "    model = XGBClassifier(enable_categorical=True, max_depth=value)\n",
    "    model.fit(X_train, y_train=='>50K')\n",
    "    scores.append(accuracy_score(y_test=='>50K', model.predict(X_test)))\n",
    "    training_scores.append(accuracy_score(y_train=='>50K', model.predict(X_train)))\n",
    "\n",
    "pd.DataFrame({'max_depth': values, 'accuracy': scores, 'training_accuracy': training_scores}).set_index('max_depth').plot(title='XGBoost Classifier Accuracy vs Depth of Trees', ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_tuned = XGBClassifier(enable_categorical=True, max_depth=3)\n",
    "xg_tuned.fit(X_train, y_train=='>50K')\n",
    "accuracy = accuracy_score(y_test=='>50K', xg_tuned.predict(X_test))\n",
    "print(f'Model Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Challenge: Tune XGBoost Model\n",
    "\n",
    "What value of `gamma` maximizes accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Solution: Tune XGBoost Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "## Concept of Stacking\n",
    "Stacking, also known as stacked generalization, is an ensemble machine learning algorithm. Unlike other ensemble methods, which are typically based on a single type of model, stacking focuses on combining different base models to improve predictive performance. The core idea is to train a meta-model (also known as a blender or second-level model) to combine predictions from multiple base models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hands-on Coding with Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# Define base models\n",
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier(max_depth=3)),\n",
    "    ('nn', KNeighborsClassifier(n_neighbors=3)),\n",
    "]\n",
    "model = StackingClassifier(estimators=estimators)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "\n",
    "model.fit(pipeline.fit_transform(X_train), y_train)\n",
    "accuracy = accuracy_score(y_test, model.predict(pipeline.transform(X_test)))\n",
    "print(f'Model Accuracy: {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Stacking vs Individual Models\n",
    "\n",
    "Now, let's compare the performance of the stacking model with its individual base models. Observe how the stacked model often outperforms the single models by combining their strengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate each individual model\n",
    "for name, clf in estimators:\n",
    "    clf.fit(pipeline.fit_transform(X_train), y_train)\n",
    "    test_score = clf.score(pipeline.transform(X_test), y_test)\n",
    "    print(f\"{name} model score: {test_score:.4f}\")\n",
    "\n",
    "# Evaluate stacking model\n",
    "test_score = model.score(pipeline.transform(X_test), y_test)\n",
    "print(f\"Stacking model score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge: Create a Stacked Model\n",
    "Try creating your own stacked model using different base models. You can experiment with models like Support Vector Machines, Decision Trees with different depths, or even different ensemble methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Solution: Create a Stacked Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
