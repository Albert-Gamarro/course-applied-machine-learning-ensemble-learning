{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83a18d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_project_root\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from src.models.utils.common import get_project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "experiment_name = \"v1_initial_run\"\n",
    "results_dir = os.path.join(get_project_root(), \"results\", experiment_name)\n",
    "results = pd.read_csv(os.path.join(results_dir, \"optuna_results.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a94fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out overfitted trials s\n",
    "non_overfitted = results[\n",
    "    (results[\"overfit\"] < 0.05)\n",
    "    & (\n",
    "        results[\"model\"] != \"RandomForest\"\n",
    "    )  # Exclude RandomForest due to consistent overfitting (not generalizing well)\n",
    "    & (\n",
    "        results[\"mean_test_precision\"] > 0.78\n",
    "    )  # Marketing: minimize false positives\n",
    "].copy()\n",
    "print(f\"\\nNumber of non-overfitted trials: {len(non_overfitted)}\")\n",
    "\n",
    "print(non_overfitted[\"model\"].value_counts())\n",
    "\n",
    "# Calculate robust F1\n",
    "non_overfitted[\"robust_f1\"] = (\n",
    "    non_overfitted[\"mean_test_f1\"] - 0.3 * non_overfitted[\"overfit\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 per model by robust F1\n",
    "top_models = []\n",
    "for model in non_overfitted[\"model\"].unique():\n",
    "    model_top = non_overfitted[non_overfitted[\"model\"] == model].nlargest(\n",
    "        3, \"robust_f1\"\n",
    "    )\n",
    "    top_models.append(model_top)\n",
    "top_models = pd.concat(top_models).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a832bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse params for readability\n",
    "\n",
    "params_df = pd.json_normalize(top_models[\"params\"])\n",
    "top_models = pd.concat([top_models.drop(columns=\"params\"), params_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summary table\n",
    "summary_cols = [\n",
    "    \"model\",\n",
    "    \"mean_test_f1\",\n",
    "    \"mean_test_accuracy\",\n",
    "    \"mean_test_precision\",\n",
    "    \"mean_test_recall\",\n",
    "    \"overfit\",\n",
    "    \"params\",\n",
    "]\n",
    "\n",
    "non_overfitted.groupby(\"model\")[summary_cols].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot: Robust F1 vs Precision\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=top_models,\n",
    "    x=\"robust_f1\",\n",
    "    y=\"mean_test_precision\",\n",
    "    hue=\"model\",\n",
    "    size=\"mean_test_accuracy\",\n",
    "    sizes=(50, 200),\n",
    ")\n",
    "plt.axhline(0.78, color=\"red\", linestyle=\"--\", label=\"Precision Threshold (business)\")\n",
    "plt.xlabel(\"Robust F1 (F1 - Overfit)\")\n",
    "plt.ylabel(\"Test Precision\")\n",
    "plt.title(\"Top Models: Robust F1 vs Precision\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f5407",
   "metadata": {},
   "source": [
    "Model Selection\n",
    "- Non-overfitted trials: {len(non_overfitted)} (GradientBoosting/XGBoost, precision > 0.8).\n",
    "- Top models: {len(top_models)} (3 per model by robust F1).\n",
    "- Selected for refinement: 3 (2 XGBoost, 1 GradientBoosting).\n",
    "- Criteria: High robust F1, precision > 0.8 for marketing.\n",
    "- Recommendation: XGBoost leads; GradientBoosting competitive, included for refinement.\n",
    "- Next: Hyperparameter analysis in `hyperparameter_analysis.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top models for refinement\n",
    "selected_models = pd.concat(\n",
    "    [\n",
    "        top_models[top_models[\"model\"] == \"XGBoost\"].nlargest(2, \"robust_f1\"),\n",
    "        top_models[top_models[\"model\"] == \"GradientBoosting\"].nlargest(1, \"robust_f1\"),\n",
    "    ]\n",
    ").reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
